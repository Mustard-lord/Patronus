python ../../diffusion_antifinetune_dataparallel.py --alpha 1 --beta 0.5 --gpus "2,3" --epochs 5000 --root "./results_antift" --batches 5 --strategy "anti_finetune" --checkpoint ../../code/bpalm/results_antift/no_detach/9_28_14_28_34/checkpoints/ckpt_1999/LoRA_unet --sample "random" --shots 12 --module "lora" --lr 0.0001 --LoRAmodule "to_q,to_k,to_v,to_out.0" --rank 8 --alpha_LoRA 8 -resume 1999